{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Zip Model MetricsÂ¶\n",
    "Use zip to write a for loop that creates a string specifying the model name and its corresponding metrics (accuracy, precision, recall) and appends it to the list model_metrics.\n",
    "\n",
    "Each string should be formatted as model: accuracy, precision, recall. For example, the string for the first model should be Model1: 0.95, 0.94, 0.93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/sbin/python3.13 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracy = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "precision = [0.94, 0.88, 0.91, 0.86, 0.92]\n",
    "recall = [0.93, 0.87, 0.91, 0.85, 0.91]\n",
    "model_metrics = []\n",
    "\n",
    "# write your for loop here\n",
    "# TODO\n",
    "# use zip in a for loop\n",
    "for model, acc, prec, rec in zip(model_names, accuracy, precision, recall):\n",
    "    model_metrics.append(f\"{model}: {acc}, {prec}, {rec}\")\n",
    "    print(model_metrics)\n",
    "\n",
    "\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [\"Model1: 0.95, 0.94, 0.93\", \"Model2: 0.89, 0.88, 0.87\", \"Model3: 0.92, 0.91, 0.91\", \"Model4: 0.87, 0.86, 0.85\", \"Model5: 0.93, 0.92, 0.91\"]\n",
    "if model_metrics == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Zip Metrics to Dictionary\n",
    "Use zip to create a dictionary model_performance that uses model_names as keys and accuracies as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracies = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "\n",
    "model_performance = {} \n",
    "# write your for loop here\n",
    "for model, acc in zip(model_names, accuracies):\n",
    "    model_performance[model] = acc\n",
    "    print(model_performance)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = {\"Model1\": 0.95, \"Model2\": 0.89, \"Model3\": 0.92, \"Model4\": 0.87, \"Model5\": 0.93}\n",
    "if model_performance == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Unzip Model Metrics\n",
    "Unzip the model_performance tuple into two model_names and accuracies tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = ((\"Model1\", 0.95), (\"Model2\", 0.89), (\"Model3\", 0.92), (\"Model4\", 0.87), (\"Model5\", 0.93))\n",
    "\n",
    "# define model_names and accuracies here\n",
    "model_names, accuracies = # TODO\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer_names = (\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\")\n",
    "correct_answer_accuracies = (0.95, 0.89, 0.92, 0.87, 0.93)\n",
    "if model_names == correct_answer_names and accuracies == correct_answer_accuracies:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Enumerate Model Performances\n",
    "Use enumerate to modify the model_descriptions list so that each element contains the model name followed by its corresponding accuracy. For example, the first element of model_descriptions should change from \"Model1 Description\" to \"Model1 Description 0.95\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = ((0.95, 0.94, 0.93), (0.89, 0.88, 0.87), (0.92, 0.91, 0.90), (0.87, 0.86, 0.85))\n",
    "\n",
    "# Transpose the matrix\n",
    "metrics_data_transpose = # TODO\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [(0.95, 0.89, 0.92, 0.87), (0.94, 0.88, 0.91, 0.86), (0.93, 0.87, 0.90, 0.85)]\n",
    "if metrics_data_transpose == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
